{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: CIFAR10 Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the data\n",
    "(X_train, y_train), (X_test, y_test) = data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the values in numpy arrays \n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the 10 unique classes\n",
    "np.unique(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different classes, the different models variables belong to:\n",
    "\n",
    "(Label) -> (Class)\n",
    "\n",
    "0 -> Airplane\n",
    "\n",
    "1 -> Automobile\n",
    "\n",
    "2 -> Bird\n",
    "\n",
    "3 -> Cat\n",
    "\n",
    "4 -> Deer\n",
    "\n",
    "5 -> Dog\n",
    "\n",
    "6 -> Frog\n",
    "\n",
    "7 -> Horse\n",
    "\n",
    "8 -> Ship\n",
    "\n",
    "9 -> Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the labels\n",
    "\n",
    "label = np.concatenate((y_train,y_test), axis = 0)\n",
    "\n",
    "# counting elements in each category\n",
    "unique, count = np.unique(label, return_counts=True)\n",
    "\n",
    "\n",
    "\n",
    "# plotting the barplot\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.barplot(x=unique,y = count )\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel(\"Number of values\")\n",
    "plt.title('How many different categories exist in the entire dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well balanced the dataset is:\n",
    "\n",
    "Based on what we see in the above plot; the dataset is very well balanced as each category contains the equal number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the test set into test and validation sets\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.7)\n",
    "\n",
    "# we split the data into 70 and 30% into test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the data generators\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(x_val, y_val, batch_size=32)\n",
    "test_generator = test_datagen.flow(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the very first convilutional layer to the model\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a pooling layer to the model\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a second convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding another pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a flatten layer to the model\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the last fully connected layer to the model\n",
    "model.add(Dense(185, activation='relu'))\n",
    "\n",
    "# Niw, adding the output layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 175)               1925      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1760      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 185)               2035      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1860      \n",
      "=================================================================\n",
      "Total params: 315,722\n",
      "Trainable params: 315,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model and printing its summary\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1562 steps, validate for 218 steps\n",
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 120s 77ms/step - loss: 1.6187 - accuracy: 0.4006 - val_loss: 1.2085 - val_accuracy: 0.5859\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 70s 45ms/step - loss: 1.2936 - accuracy: 0.5556 - val_loss: 1.1350 - val_accuracy: 0.6141\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 48s 31ms/step - loss: 1.2178 - accuracy: 0.5814 - val_loss: 1.0933 - val_accuracy: 0.6340\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 47s 30ms/step - loss: 1.1841 - accuracy: 0.5936 - val_loss: 1.0617 - val_accuracy: 0.6469\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 49s 31ms/step - loss: 1.1554 - accuracy: 0.6051 - val_loss: 0.9850 - val_accuracy: 0.6659\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 50s 32ms/step - loss: 1.1256 - accuracy: 0.6114 - val_loss: 0.9636 - val_accuracy: 0.6736\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 486s 311ms/step - loss: 1.1102 - accuracy: 0.6180 - val_loss: 0.9321 - val_accuracy: 0.6868\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 156s 100ms/step - loss: 1.0846 - accuracy: 0.6239 - val_loss: 0.9714 - val_accuracy: 0.6746\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 122s 78ms/step - loss: 1.0728 - accuracy: 0.6318 - val_loss: 0.9209 - val_accuracy: 0.6833\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 113s 73ms/step - loss: 1.0498 - accuracy: 0.6368 - val_loss: 0.9262 - val_accuracy: 0.6831\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history_CNN = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // 32,\n",
    "    epochs=10,  \n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(x_val) // 32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, len(X_train) // 32 calculates the total number of batches in the training set. Similarly, for validation data, validation_steps calculates the number of batches in the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now using a model with pretrained bias from resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the resnet model wothout the top layer\n",
    "\n",
    "model_2 = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an ImageDataGenerator for training with data augmentation\n",
    "resnet_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "model_2_val_generator = val_datagen.flow(x_val, y_val, batch_size=32)\n",
    "model_2_test_generator = test_datagen.flow(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 (Residual Networks)\n",
    "\n",
    "\n",
    "### The new chosen model is a resnet model\n",
    "We think it would work better because of its high performance due to pretrained biases and other model hyperparameters; \n",
    "- **Flexible**: Any number of new layers can be added on the top to increase model accuracy\n",
    "- **Residual learning**: We get the benefit of the previous knowledge and training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will freeze the first 140 layers of the pretrained model\n",
    "for layer in model_2.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Sequential()\n",
    "new_model.add(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new layers on the top and base of the model\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(18, activation='relu'))\n",
    "new_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the new resnet model\n",
    "\n",
    "new_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, training the newly build model\n",
    "\n",
    "history = new_model.fit(\n",
    "    model_2_train_generator,\n",
    "    steps_per_epoch=len(X_train) // 32,\n",
    "    epochs=10,  \n",
    "    validation_data=model_2_val_generator,\n",
    "    validation_steps=len(x_val) // 32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the confusion matrix for tranied CNN model: history_CNN\n",
    "# getting the training and validation accuracies\n",
    "\n",
    "train_acc_CNN = history_CNN.history['accuracy'][-1]\n",
    "val_acc_CNN = history_CNN.history['val_accuracy'][-1]\n",
    "\n",
    "# predictions on the validation dataset\n",
    "val_pred_CNN = np.argmax(model.predict(val_generator), axis=1)\n",
    "\n",
    "# computing the cnfusion matrix\n",
    "con_mat_CNN = confusion_matrix(y_val, val_pred_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix and result for the manually created CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the training and validation accuracies\n",
    "print(\"For manully trained CNN model \\n\")\n",
    "print('\\n')\n",
    "print(\"Training accuracy: \", train_acc_CNN)\n",
    "print('\\n')\n",
    "print(\"Validation accuracy: \", val_acc_CNN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(con_mat_CNN, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Custom CNN Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the confusion matrix for transfer learning resnet50 model: history\n",
    "# getting the training and validation accuracies\n",
    "\n",
    "train_acc_res = history.history['accuracy'][-1]\n",
    "val_acc_res = history.history['val_accuracy'][-1]\n",
    "\n",
    "# predictions on the validation dataset\n",
    "val_pred_res = np.argmax(new_model.predict(model_2_val_generator), axis=1)\n",
    "\n",
    "# computing the cnfusion matrix\n",
    "con_mat_CNN = confusion_matrix(y_val, val_pred_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix and result for the transfer learning (Resnet50) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the training and validation accuracies\n",
    "print(\"\\n For transfer resnet50 model \\n\")\n",
    "print('\\n')\n",
    "print(\"Training accuracy: \", train_acc_res)\n",
    "print('\\n')\n",
    "print(\"Validation accuracy: \", val_acc_res)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(con_mat_CNN, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Custom CNN Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on the best chosen model amongst the two:\n",
    "\n",
    "We had two models, the manually trained using convolutional layers and the one trained with frozen layers, we find that the manullay trained model performs better here with the training accuracy of **66.5%** and validation accuracy of **68%**.\n",
    "\n",
    "However, the resnet model could only achieve the training accuracy of **45.75%** and validation accuracy of **32.3%**, which is comparitively lower, this doesn't mean the model is not trained well, it could be due to the following reasons:\n",
    "\n",
    "- less number of dense layers added to the transferred model that is trained with all its previous layers frozen; \n",
    "- Or, it might be becuase the model is trained to process different set of images than the one we are using to on however, we used preprocess input function properly with our data before training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the testing accuracy of the model: CNN\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "# making predictions on the test dataset\n",
    "y_test_pred_CNN = np.argmax(model.predict(test_generator), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and confusion matrix for test data for CNN model(manually trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, computing and plotting the confusion matrix:\n",
    "print(\"\\n The test accuracy of CNN model manually trained is: \", test_accuracy)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "con_mat_test_CNN = confusion_matrix(y_test, y_test_pred_CNN)\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(con_mat_test_CNN, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for test dataset Custom CNN Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocessing the images to have the valid input size\n",
    "def preprocess(image_sent):\n",
    "    img = image.load_img(image_sent, target_size=(32, 32))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "    return img_array\n",
    "\n",
    "image_sent = [r'C:\\Users\\gilld\\OneDrive\\Documents\\RRC\\Winter_Term_2025\\COMP-3704 Neural Networks and Deep Learning\\Assignment_3\\im1.jpg', r'C:\\Users\\gilld\\OneDrive\\Documents\\RRC\\Winter_Term_2025\\COMP-3704 Neural Networks and Deep Learning\\Assignment_3\\bird.jpg', r'C:\\Users\\gilld\\OneDrive\\Documents\\RRC\\Winter_Term_2025\\COMP-3704 Neural Networks and Deep Learning\\Assignment_3\\cat.jpg']\n",
    "\n",
    "images = np.vstack([preprocess(image_sent) for image_sent in image_sent])\n",
    "\n",
    "# making predictions on additional images\n",
    "predictions = model.predict(images)\n",
    "\n",
    "# getting the classes\n",
    "classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# printing the valid classes\n",
    "names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "for i, image_sen in enumerate(image_sent):\n",
    "    print(f\"The Image {image_sen} is predicted to be {names[classes[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predictions on different images\n",
    "\n",
    "- The first image was an airplane and is predicted to be a truck\n",
    "\n",
    "- The second image was bird and is predicted to be a bird\n",
    "\n",
    "- The third image was a cat image and is predicted to be an automobile\n",
    "\n",
    "So, here we only get one correct answers for three images, so our model doesn't perform really well, so the accuracy is 33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
